\chapter{Related Work}
This chapter reviews related work looking at defining Big Data in terms of DSNs, Big Data management, self-optimizing DSNs, predictive analytics and forecasting, optimizations to triggering, detection, and classification of signals-of-interest within the context of DSNs.

\section{Big Data and Distributed Sensor Networks}
Big Data is a term that is used to define either the characteristics of collected data or the processes involved for storing and analyzing collected data. Information that is considered Big Data provides a number of challenges.

One of the best and somewhat up-to-date (2014) reviws on Big Data literature is provided by the Presidentâ€™s Council of Advisors on Science and Technology (PCAST) in their report to the White House\cite{house2014big}. In this review, Big Data is described using several definitions. 

The first definition includes ``high-volume, high velocity, and high-variety information assets that demand cost-effective, innovative forms of information processing for enhanced insight and decision making"\cite{gartner_it_glossary_2016}. This definition focuses on the characteristcs of the data that make it ``Big". In this context high-volume refers to the total amount of data that requires processing, high-velocity refers to the speed at which data arrives, and high-variety refers to the fact that sensor data is often heterogenous and incomplete. The second part of the defintion includes the terms cost-effective, innovative forms of information processing for enhanved insight and decision processing which hints at the fact that we need technology that is able to deal with these types of data characterisics while doing so within the limits of a system with the goal of refining the data to provide insights and decision making that wouldn't have been possible without the information processing. 

A second definition\cite{ward2013undefined}  mentioned by the PCAST report rings more true to what Laha attempts to accomplish within the context of DSNs and says that Big Data ``a term describing the storage and analysis of large and/or complex data sets using a series of techniques including, but not limited to, NoSQL, MapReduce, and machine learning". This second definition defines Big Data in terms of stroage and analysis techniques and is a useful definition for describing the processes by which Laha and the Laha reference DSNs deal with distributed sensor data.

\section{Optimizations for Triggering}
Triggering is the act of observing a feature extracted data stream for interesting features and triggering sensors to provide raw data for a requested time window for higher level analysis. Adaptively optimizing triggering is a way to tune triggering algorithms and parameters with the aim of decreasing false positives and false negatives. In this context, a false positive is triggering on a data stream that does not contain a signal of interest and a false negative is not triggering on a data stream that does contain a signal of interest. 

Many of the optimizing triggering algorithms present in the literature exist to minimize sensor energy requirements and bandwidth requirements. This is addressed in great detail in the literature review by Anastasi et al. \cite{anastasi_energy_2009}. This is accomplished by reducing communications between sensor nodes and the sink. It's argued in \cite{pottie2000wireless} that the cost of transmitting a single bit of information from a sensor cost approximately the same as running 1000 operations on that sensor now. However, there is some contention on this topic as \cite{alippi_adaptive_2010} argues that in some modern sensors computational requirements can equal or eclipse those of  sensor communication.  

One of the main drivers of optimization of triggering is to take advantage of the known sensing field topology of a DSN. This is often refered to in the literature as ``toplogy control"\cite{santi2005topology}. When the topology of the sensing field is known and when there is an adequate density of sensors, Vuran et al. show that sampled data display strong spatial and temporal correlations\cite{vuran2004spatio}. This fact can be used to reduce the amount of duplicate sensor data that is transmitted, stored, and processed. Toplogy control is generally split into two categories, ``location  driven" where the location of the sensor is known and ``connectivity driven" which aims to dynamically activate or deactivate sensors to provide complete coverage of a sensing field. Many of the location based approaches in the literature attempt  to maximize the ability for sensors to communicate with each other, however Laha takes the approach that all sensors communicate directly with sink nodes eliminating the need for optimizing intra-sensor communications. One downside to location based approaches is that GPS sensors can be energy hogs and only work with directly line of site to the atmostphere. In these cases, a subset of sensors can be supplied with a GPS and the other sensor use additional tecniques such as NTP or statistical analysis to determine location\cite{langendoen2003distributed}.

More details on toplogy control can be gathered in the reviews by Karl et al.\cite{karl2007protocols} and Vuran et al.\cite{vuran2004spatio}.

\section{Distributed Sensor Networks and Big Data Management}

\section{Distributed Sensor Networks and Predictive Analytics and Forecasting}
\cite{anastasi_energy_2009} breaks data predictions algorithms for DSNs into two classes. The first class of algorithms are defined as stochastic approaches and use probabilities and statistics to provide predictions. The other class is called time series forecasting and uses histoical time series data to provide future predictions. An example of a stochastic model for predicting sensor data is the Ken model\cite{chu2006approximate} which was developed for energy reduction by minimizng the data sent between sensors and sink nodes. This is accomplished by using a model of sensed data and only sending data when the sensed values at the sensor do not match what was predicted by the model. The model is built during a training phase in which a probabilistic density function (PDF) is generated for the model. Ken is flexible enough to provide models for different types of sensed phenomenon and can work anywhere where there are high correlations in time and space.

Time series forecasting algorithms typically use moving average, auto regressive, or auto regressive moving average models. The authors of the PAQ framework\cite{tulone2006paq} uses auto-regression tecniques to build a model of sensor readings that is compared betwen sensor node and sink nodes while  providing probably correct error bounds. The SAF architecture\cite{tulone2006energy}, by the same authros, improves on the PAQ framework by refining the AR models and also adds the ability to not only detect outliers, but also detect inconsistent data. These approaches provide predictions for a single feature, however Laha provides the ability for DSNs to be multi-modal. The paper presented by Le et al.\cite{le2007adaptive} uses time series forecasting, but provides multiple models which are switched out when the data changes. That is, given the current state of the network, a model is selected that is most likely to provide correct predictions. This is useful if a network has multiple features that can be used for forecasting.

\section{Determining Toplogy and Localization}
The paper by Langendoen and Reijers\cite{langendoen2003distributed} provides comparisons for localization techniques of large DSNs. Langendoen's requires that the approaches are self organizing and do not depend on global infastructure (such as GPS) , are tolerant to node failures, and are energy efficient. These constraints rule out other localization approaches such as GPS. One thing that differentiates Laha networks to Langendoen's is that Langendoen assumes a random distribribution of sensor nodes where sensors in Laha networks are strategically placed. If there are a fraction of nodes that do know their location (anchor nodes), then there are several techniques that meet Langendoen's criteria including Ad hoc Positioning System from Niculescu et al.\cite{niculescu2003ad}, the N-hop Multilateration Primitive by Savvides et al.\cite{savvides2002bits}, and Rabaey's work on robust positioning algorithms\cite{rabaey2002robust}. The three approaches all use three similar phases for localization: distance between anchor nodes and other sensors, position, and refinement. Laha hopes to provide sensor distance between sensors rather than phsyical distance. The above algorithms use flooding of the network for evaluate distance metrics, which may not be possible in Laha deployed networks.

When timing synchronization between nodes is sufficient, that is, the synchronization between sensors provides a timing accuracy of more than the Nyquist frequency for the signals of interestest trying to be captured, it's possible to use arrival time of signals to provide metrics on sensing field topology and localization. This is the premise behind sets of algorithms that look at a single signal and the arrival times of that signal at multiple sensors along with possible direction and then attempt to provide an estimate of source signal localization. This has been performed in infrasound networks using the INFERNO framework as described by Perttu\cite{perttu2013regional} and in other accoustic DSNs such as those used for efficient shooter localization (finding the source of a gun shot from collected accostic signatures) in \cite{gezici2005localization} and \cite{maroti2004shooter}. Localization of non-accoustic signals has also been shown in the literature. For example, Parsons el al. provide a method for localizing PQ disturbances by analyzing energy flow and peak instantaneous power for both capacitor energizing  and voltage sag disturbances from sampled voltage and current data\cite{parsons1998direction}.

Although not related to determining the toplogy of a PQ network, there is research that can also find the optimal placement of PQ sensors given a the toplogy of the network. Won, et al.\cite{won2008optimal} provide an automatic method of placing PQ sensors on a known topology to maximize signal collection while minimizing the total number of required sensors.  








