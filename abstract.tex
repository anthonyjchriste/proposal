\begin{abstract}
	Distributed Sensor Networks (DSNs) are faced with a myriad of technical challenges. This dissertation examines two important DSN challenges. 
	
	One problem that is apparent in any DSN is converting ``primitive" sensor data into actionable products and insights. To make progress towards this problem, DSNs typically implement one or more of the following strategies: detecting signals in the primitive data (deciding if something is there), classification of signals from primitive data (deciding what is there), localization of signals (when and where did the signals come from), and by forming relationships between primitive data by finding correlations between spatial attributes, temporal attributes, and by associating metadata with primitive data to provide contextual information not collected by the DSN. These strategies can be employed recursively. As an example, the result of aggregating typed primitive data provides a new higher level of types data which contains more context than the data from which is was derived from. This new typed data can itself be aggregated into new, higher level types and also participate in relationships.
	
	A second important challenge is managing data volume. Most DSNs produce large amounts of (increasingly multimodal) primitive data, of which only a tiny fraction (the signals) is actually interesting and useful. The DSN can either utilize one of two strategies: keep all of the information and primitive data forever, or employ some sort of strategy for systematically discarding (hopefully )uninteresting and not useful) data. As sensor networks, the first strategy becomes unfeasible. Therefore, DSNs must find and implement a strategy for managing large amounts of sensor data. The difficult part is finding a trade off between deciding what data is interesting and must be kept and what data to discard.
	
	This dissertation investigates the design, implementation, and evaluation of the Laha framework, which is intended to address both of these problems. First, the Laha framework provides a multi-leveled representation for structuring and processing DSN data. The structure and processing at each level is designed with the explicit goal of turning low-level data into actionable insights. Second, each level in the framework implements a ``time-to-live" (TTL) strategy for data within the level. This strategy states that data must either ``progress" upwards through the levels towards more abstract, useful representations within a fixed time window, or be discarded and lost forever. The TTL strategy is interesting because when implemented, it allows DSN designers to calculate upper bounds on data storage at each level of the framework and supports graceful degradation of DSN performance.
	
	The claim of this dissertation is that the Laha Framework provides a generally useful representation for DSNs. I will evaluate this claim in the following ways.
	
	First, to evaluate the generality of the network, I will design, implement, and deploy two Laha-compliant reference networks in two different domains, power quality and infrasound. These  reference implementations will generate evidence for the ways in which Laha supports the goals of the sensor networks and ways in which it might fall short. The implementations may also provide insights into the types of distributed sensor networks for which Laha is well-suited, and the types for which it is not.
	
	Second, these implementations will enable me to evaluate the multi-level representation system. I claim that Laha will enable a distributed sensor network to derive actionable insights from low level data, and that each of the levels will be important to that process. The two reference implementations will provide concrete data as to the set of levels that are useful in practice, or whether different levels would be more appropriate, or if the level strategy itself has problematic features.
	
	Third, my evaluation will assess the TTL-based approach to managing data volume. I claim that a benefit of Laha's mechanism for managing data is that it will enable the calculation of upper bounds on data storage requirements. In my evaluation, I will develop the analytical procedures required for calculating data storage requirements, and see if these procedures are valid in practice.  One obvious problem with a TTL approach is the possibility of false negatives: data that is discarded before it has been recognized as important. My evaluation will include studies designed to assess the frequency of false negatives and how important the problem might be in practice.
\end{abstract}