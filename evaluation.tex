\chapter{Evaluation}
Evaluation of the Laha framework involves deploying reference Laha-compliant DSNs, validating the data collected from the reference implementations, and then comparing and contrasting various metrics for each of the seven proposed benefits over the course of a set of experiments designs to feature each of the benefits that Laha proposes.

\section{Deploy Laha reference implementations on test sites}
In Q4 2018, 10 to 20 Laha-compliant OPQBoxes will be deployed on the University of Hawaii at Manoa's power microgrid. Using a provided blueprint of the microgrid as a guide and collaborating with the Office of Energy Management, these sensors will be placed strategically with the hopes of observing PQ signals on the same line, PQ signals generated from intermittent renewables, local PQ signals, global PQ signals, and PQ signals near sensitive lab electronics. Many of these sensors will be co-located with industry standard PQ monitoring systems. The industry standard sensors provide both ground truth and a means of comparison between a Laha powered network and a normal DSN. 

In Q4 2018,  20 to 30 Laha-compliant Lokahi sensors will be deployed near and around the Infrasound Laboratory in Kailua-Kona on the Big Island of Hawaii. These sensors will be placed strategically around a calibrated infrasound source. The sensors will be placed with the assistance of Dr. Milton Garces to ensure that we can target sensors at different distances by tuning the amplitude and frequencies of the infrasound signal. In this way, we know which devices should or should not have received the signal.

\section{Validate data collected by Laha deployment}
Beginning in Q1 2019, I will begin validated data collection from both the OPQ network and the Lokahi network. 

Data will be validated in the OPQ network be comparing detection and classified signals against industry standard meters that are co-located with our sensors. Data validation will be an autonomous process that validates signals seen in both the industry sensor and the OPQ sensor and also provides a means of quantifying the amount of signals that our sensors were not able to detect. 

Data from the Lokahi network will be validated against industry standard infrasound sensors. We also control the amplitude and frequency of the signals generated from the calibrated infrasound source and can use geophysical equations to predict which sensors should have seen or not seen an infrasonic signal. Data validation is autonomous for this network.

Data validation for both networks will continue for all data collection until the end of the project.

\section{Use Laha deployments to evaluate each of the seven proposed benefits discussed in section \ref{laha-benefits}}
The Laha deployments for both OPQ and Lokahi will be used to evaluate each of the proposed benefits discussed in section \ref{laha-benefits}. Each deployment offers different techniques for performing evaluation. 

In the OPQ deployment, OPQBoxes are deployed and co-located with industry standard, calibrated, reference sensors. Each of these sensors cost thousands to obtain and install, collect all the data all the time, and can only be connected to the power main as it enters a building. [TODO discuss the actual hardware and characteristics of the sensors installed by the office of power management at uh here]. These sensors provide a means for verifying signals received or not received by OPQ, as well as confirming long term trend data. I have been provided access to these sensors and stored data via the Office of Energy Management at UH Manoa. The data is accessible via an HTTP API. The Office of Energy Management at UH Manoa has also provided the full schematics for the UH power grid. This will be used as a ground truth for topology estimates and distributed signal analysis. OPQBoxes are placed in strategic locations on the UH Manoa campus specifically in order to evaluate the distributed nature of PQ signals. For example, OPQBoxes are placed on the same electrical lines as well as separate electrical lines to observe how PQ signals travel through an electrical grid.
% TODO discuss the actual hardware and characteristics of the sensors installed by the office of power management at uh here

In the Lokahi deployment, we have the opportunity to generate infrasound signals using a calibrated infrasound source. The source can be tuned to produce infrasound at configurable frequencies and amplitudes. The source works by attaching a variable pitch propeller to an electric motor that can be driven by a waveform generator. The source can generate signals that can be observed at large stand off distances, over tens of kilometers. Similar to the OPQ deployment, sensors within the Lokahi deployment will be co-located with industry standard, calibrated, infrasound sensors. These sensors can provide a metric of signals that were correctly observed, incorrectly observed, or not observed at all by the Lokahi deployment. [TODO characterize and provide details on these sensors] Further, infrasound itself is characterized quite well by various geophysical equations. These equations can be used to predict if sensors deployed in the Lokahi deployment are likely to observe generated infrasound signals.
% TODO discuss the actual hardware and characteristics of the sensors installed by the office of power management at uh here

\subsection{Evaluation of Tiered Management of Big Data}\label{eval-big-data}
During the acquisition and curating of data, metrics will be collected and stored about how much data is saved versus how much data is discarded at each layer within the Laha data hierarchy. These numbers will be compared against data storage as if the OPQ and Lokahi frameworks were to take a ``store everything" approach. Evaluation metrics provided will include percentage of data storage saved per data hierarchy layer as well as an estimate of overall decrease in data storage requirements for the entire DSN. 

I will also provide metrics on ``continuous storage pressure" which is a measure of the average amount of storage required at each layer given the current state of the network. That is, since data at all lower levels of the framework assigns a TTL to the data within the collection, the collection will exhibit a constant data pressure during sensor data collection. For example, at the lowest level, the IML collects raw data from all sensors all the time. Given the sample rate per sensor, the size per sample, the number of sensors, and a known TTL for this layer, we can estimate the maximum bounds of data management requirements that the IML requires. We can play similar estimation games with higher layers of the framework.

By removing data, Laha runs the risk of discarding data that contains signals of interest that our system was not able to detect or classify. 

By controlling the signals generated within the Lokahi DSN deployment, we can provide a measure of signals that were thrown out and not detected by our framework that would have been preserved if our framework took the approach of ``collect everything all the time". Similarly, calibrated reference sensors co-located with the OPQ deployment can provide metrics on signals that were observed by the reference sensors and subsequently thrown away and not observed within OPQ.

\subsection{Evaluation of Contextualizing Classified Incidents}
% TODO
[TODO: This is probably my weakest evaluation strategy of the 7 and I will continue to fill this one out this week].

To evaluate contextualized events, we will provide known. cyclical signals for both Laha deployments. We will first provide context for signals in both deployments as training data for the DSNs. Then, at difference distances from the source, I will reproduce the signals and will then provide a statistical error analysis for whether or not Laha is able to correctly contextualize the signals.

\subsection{Evaluation of Adaptive Optimizations for Triggering}
In order to evaluate triggering efficiency within our Laha deployments, Laha will only adaptively modify triggering for half of the devices the OPQ deployment. In the Lokahi deployment, we will run the same experiment twice. The first run will not optimize triggering and the second run will optimize triggering. Then, the following metrics will be tabulated to provide an evaluation for triggering efficiency:

\begin{itemize}
	\item Number of triggers performed from optimized devices versus number of triggers from  unoptimized devices
	\item Bandwidth requirements from optimized triggering versus bandwidth requirements from unoptimized triggering
	\item Storage requirements as a result of event storage from optimized triggering versus storage requirements from unoptimized triggering
	\item Detection, classification, and incident counts for optimized triggering versus detection, classification, and incident counts for unoptimized triggering
\end{itemize}

\subsection{Evaluation of Adaptive Optimizations for Detection and Classifications}
Evaluation of adaptive optimizations for detection and classification within the Laha network will be conducted differently for each Laha deployment.

In the Lokahi deployment, I will control the production of infrasound signals using the available infrasound source. I will run two experiments, where the amplitudes and frequencies of the signals are the same and the locations of the devices remain invariant. In the first experiment, Laha will not use optimized detection or classification provided by Phenomena. In the second experiment, Laha will use optimized detection and classification techniques provided by Phenomena. 

With known frequencies and amplitudes of the infrasound signals, we can compare the rate of detections and classifications between the optimized and unoptimized experimental runs. I expect to see a greater number of and more accurate detections and classifications from the optimized experiment.

In the OPQ deployment, we will compare the same metrics as the Lokahi deployment, but instead of controlling the source signal, we will co-locate OPQBoxes. In each pair of co-located OPQBoxes, one will be analyzed using Phenomena optimized detection and classification algorithms and the other will be analyzed using unoptimized detection and classification algorithms.

\subsection{Evaluation of Model of Underlying Sensor Field Topology}
To evaluate the model of the sensing field topology, I will take two different approaches for each Laha deployment.

In the Lokahi deployment, sensors will be strategically placed at different distances from the infrasound source. Some sensors will be close to each other geographically, but separated by terrain that infrasound signals will not easily travel through. By moving the infrasound source, we can expect to see infrasound signals arriving or not arriving at the sensors depending on the source and direction of the signal along with the physical features of the land. By performing multiple experiments, I hope to provide a model of the physical environment topology that Laha has built. I will compare Laha's model to the known topology and provide a statistical error analysis. 

In the OPQ deployment, sensors will be strategically placed on like and unlike electrical lines to observe how distributed PQ signals move through a power grid. In this deployment, Laha will build a topology model that doesn't show physical geographic distance between sensors, but instead will build a model of the electrical distance between sensors. This data will be evaluated by comparing the electrical distances found by the Laha model to the actual UH power grid as referenced by the schematic provided by the Office of Energy Management at UH Manoa. A statistical error analysis of the differences between electrical distances between the model and the schematic will be provided as an evaluation metric.

\subsection{Evaluation of Decreased Bandwidth of Entire DSN}
Bandwidth of the entire DSN can be evaluated by counting the number of bytes sent and received by the sink nodes in both Laha deployments. 

In the Lokahi deployment, I will run two separate experiments. The first without any Phenomena optimizations, and the second with all Phenomena optimizations. All other conditions (amplitude, frequencies, sensor locations) will remain invariant between experimental runs. We will then compare the total bandwidth used between experimental runs.

For the OPQ deployment, I will co-locate sensors with Phenomena optimizations applied for one of the two and Phenomena optimizations not applied to the other of the two. I will then compare total bandwidth usage between sensors that had optimizations versus those that did not utilize optimizations.

\subsection{Evaluation of Decreased Sensor Device Power Requirements of Entire DSN}
Sensor device power requirements will be evaluated separately for both OPQ and Lokahi deployments. 

In the Lokahi network, all sensors run off battery power. I will perform two experiments. In the first experiment all Phenomena optimizations will be disabled. In the second experiment, I will enable all Phenomena optimizations. We will start each experiment with full battery power for each sensor and we will record the rate of battery drain as well as the final battery usage at the end of each experiment. The experiments will be exactly the same in terms of signals produced and sensor configurations. The only difference will be whether or not all Laha optimizations will be enabled or disabled. 

In the OPQ deployment, we will attach a power consumption meter to a pair of co-located sensors. One sensor will use Laha optimizations, the other will now. I will measure and compare power consumption between the two sensors. 